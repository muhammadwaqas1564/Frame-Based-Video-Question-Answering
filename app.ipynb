{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Run this code on Kaggle using a P100 GPU if you don't have high-end hardware resources."
      ],
      "metadata": {
        "id": "2l4rd0D44pLv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ani3Thaf3eWe"
      },
      "outputs": [],
      "source": [
        "!pip install git+https://github.com/huggingface/transformers accelerate flash_attn\n",
        "!pip install qwen_vl_utils av\n",
        "!pip install streamlit pyngrok"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import tempfile\n",
        "import torch\n",
        "\n",
        "from transformers import Qwen2VLForConditionalGeneration, AutoProcessor\n",
        "from qwen_vl_utils import process_vision_info\n",
        "\n",
        "# Load model and processor\n",
        "st.title(\"FRAME-BASED VIDEO QUESTION ANSWERING SYSTEM\")\n",
        "\n",
        "@st.cache_resource(show_spinner=False)\n",
        "def load_model_and_processor():\n",
        "    model_name = \"Qwen/Qwen2-VL-2B-Instruct\"\n",
        "    model = Qwen2VLForConditionalGeneration.from_pretrained(\n",
        "        model_name, torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\", device_map=\"auto\"\n",
        "    )\n",
        "    processor = AutoProcessor.from_pretrained(model_name)\n",
        "\n",
        "    # Check if CUDA is available and set the device accordingly\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = model.to(device)\n",
        "\n",
        "    return model, processor, device\n",
        "\n",
        "model, processor, device = load_model_and_processor()\n",
        "\n",
        "# File uploader and question input\n",
        "video_file = st.file_uploader(\"Upload a video\", type=[\"mp4\"])\n",
        "question = st.text_input(\"Enter your question\")\n",
        "\n",
        "if st.button(\"Submit\"):\n",
        "    if video_file and question:\n",
        "        # Save the uploaded video to a temporary file\n",
        "        with tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\") as temp_file:\n",
        "            temp_file.write(video_file.read())\n",
        "            temp_file_path = temp_file.name  # Capture the path\n",
        "\n",
        "        # Define the message structure\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"type\": \"video\",\n",
        "                        \"video\": temp_file_path,\n",
        "                        \"max_pixels\": 512 * 512,\n",
        "                        \"fps\": 1.0,\n",
        "                    },\n",
        "                    {\"type\": \"text\", \"text\": question},\n",
        "                ],\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Prepare the input for the model\n",
        "        text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "        image_inputs, video_inputs = process_vision_info(messages)\n",
        "\n",
        "        inputs = processor(\n",
        "            text=[text],\n",
        "            images=image_inputs,\n",
        "            videos=video_inputs,\n",
        "            padding=True,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "\n",
        "        # Move inputs to the same device as the model\n",
        "        inputs = inputs.to(device)\n",
        "\n",
        "        # Generate the response\n",
        "        with torch.no_grad():  # To avoid unnecessary gradient computations\n",
        "            generated_ids = model.generate(**inputs, max_new_tokens=512)\n",
        "\n",
        "        generated_ids_trimmed = [\n",
        "            out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, generated_ids)\n",
        "        ]\n",
        "\n",
        "        output_text = processor.batch_decode(\n",
        "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
        "        )\n",
        "\n",
        "        # Display the output text\n",
        "        if output_text:\n",
        "            st.write(output_text[0])\n",
        "        else:\n",
        "            st.write(\"No response generated.\")\n",
        "    else:\n",
        "        st.write(\"Please upload a video and enter a question.\")"
      ],
      "metadata": {
        "id": "Z_INFY5Z3r8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok authtoken \"Add your authtication ngrok token\"\n",
        "#https://dashboard.ngrok.com/get-started/your-authtoken"
      ],
      "metadata": {
        "id": "ImKlEfB730ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the Streamlit app and expose it via ngrok\n",
        "import subprocess\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Start Streamlit in the background\n",
        "process = subprocess.Popen(['streamlit', 'run', 'app.py'])\n",
        "\n",
        "# Create an ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "print(f\"Streamlit app is live at: {public_url}\")\n"
      ],
      "metadata": {
        "id": "IpYKGPXc37S_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}